{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the results of football matches of the Spanish First Division League\n",
    "# Eduardo Sthory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# import warnings filter\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from Functions import result_to_numeric, best_features\n",
    "#from Functions.ipynb import result_to_numeric, best_features\n",
    "import Functions \n",
    "from Features_Engineer import features_engineer\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: http://football-data.co.uk\n",
    "Name of the results dataset fields SOURCE:\n",
    "\n",
    "    - SP1-2009.csv\n",
    "    - SP1-2010.csv\n",
    "    - SP1-2011.csv\n",
    "    - SP1-2012.csv\n",
    "    - SP1-2013.csv   \n",
    "    - SP1-2014.csv\n",
    "    - SP1-2015.csv\n",
    "    - SP1-2016.csv\n",
    "    - SP1-2017.csv\n",
    "    - SP1-2018.csv\n",
    " \n",
    " Fields:\n",
    " \n",
    " - Div = League Division\n",
    " - Date = Match Date (dd/mm/yy)\n",
    " - HomeTeam = Home Team\n",
    " - AwayTeam = Away Team\n",
    " - FTHG and HG = Full Time Home Team Goals\n",
    " - FTAG and AG = Full Time Away Team Goals\n",
    " - FTR and Res = Full Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    " - HTHG = Half Time Home Team Goals\n",
    " - HTAG = Half Time Away Team Goals\n",
    " - HTR = Half Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    " \n",
    " Match Statistics \n",
    " \n",
    " - HS = Home Team Shots\n",
    " - AS = Away Team Shots\n",
    " - HST = Home Team Shots on Target\n",
    " - AST = Away Team Shots on Target\n",
    " - HC = Home Team Corners\n",
    " - AC = Away Team Corners\n",
    " - HF = Home Team Fouls Committed\n",
    " - AF = Away Team Fouls Committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 10 last Season 2009-2018\n",
    "data1 = pd.read_csv('SP1-2009.csv')\n",
    "data2 = pd.read_csv('SP1-2010.csv')\n",
    "data3 = pd.read_csv('SP1-2011.csv')\n",
    "data4 = pd.read_csv('SP1-2012.csv')\n",
    "data5 = pd.read_csv('SP1-2013.csv')\n",
    "data6 = pd.read_csv('SP1-2014.csv')\n",
    "data7 = pd.read_csv('SP1-2015.csv')\n",
    "data8 = pd.read_csv('SP1-2016.csv')\n",
    "data9 = pd.read_csv('SP1-2017.csv')\n",
    "data10 = pd.read_csv('SP1-2018.csv')\n",
    "\n",
    "# Eliminate unnecessary columns\n",
    "data1 = data1.iloc[:,:18]\n",
    "data2 = data2.iloc[:,:18]\n",
    "data3 = data3.iloc[:,:18]\n",
    "data4 = data4.iloc[:,:18]\n",
    "data5 = data5.iloc[:,:18]\n",
    "data6 = data6.iloc[:,:18]\n",
    "data7 = data7.iloc[:,:18]\n",
    "data8 = data8.iloc[:,:18]\n",
    "data9 = data9.iloc[:,:18]\n",
    "data10 = data10.iloc[:,:18]\n",
    "\n",
    "# Drop Div, Date and others\n",
    "data1 = data1.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data2 = data2.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data3 = data3.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data4 = data4.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data5 = data5.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data6 = data6.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data7 = data7.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data8 = data8.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data9 = data9.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "data10 = data10.drop(['Div','Date','HTHG','HTAG','HTR'],axis=1)\n",
    "\n",
    "# Transform FTR (Target)\n",
    "data1['FTR'] = data1.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data2['FTR'] = data2.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data3['FTR'] = data3.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data4['FTR'] = data4.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data5['FTR'] = data5.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data6['FTR'] = data6.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data7['FTR'] = data7.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data8['FTR'] = data8.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data9['FTR'] = data9.apply(lambda row: result_to_numeric(row),axis=1)\n",
    "data10['FTR'] = data10.apply(lambda row: result_to_numeric(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union data\n",
    "# It was decided to work with the last 4 seasons\n",
    "features = pd.concat([data1,data2,data3,data4,data5,data6,\n",
    "                      data7, data8, data9, data10], ignore_index=True)\n",
    "target = pd.concat([data1['FTR'],data2['FTR'],data3['FTR'],\n",
    "                    data4['FTR'],data5['FTR'],data6['FTR'], \n",
    "                    data7['FTR'],data8['FTR'],data9['FTR'], \n",
    "                    data10['FTR']],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get features fields more importants\n",
    "features_tmp = features[['FTHG','FTAG','HS','AS','HST','AST','HF','AF','HC','AC']]\n",
    "# Rank Best features\n",
    "tmp=best_features(features_tmp, target)\n",
    "print(tmp)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FTAG, FTHG, AST and HST characteristics are the most important, we are going to work with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot feature most important\n",
    "tmp=Functions.visualize_best_features(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "spm1 = scatter_matrix(features[['FTHG','FTAG','HS','AS','HST','AST','HF','AF','HC','AC']],\n",
    "                       alpha=0.2,figsize=(6,6),diagonal = 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = features[['FTHG','FTAG','HS','AS','HST','AST','HF','AF','HC','AC']].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import describe\n",
    "#describe(features[['FTHG','FTAG','HS','AS','HST','AST','HF','AF','HC','AC']], axis=0)\n",
    "\n",
    "import pandas_profiling as pp\n",
    "eda = pp.ProfileReport(features)\n",
    "display(eda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop characteristics with less importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop HC, AC, AS, HS, HF, AF\n",
    "features = features.drop(['HC','AC','AS','HS','HF','AF'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Taking into account the above, we will take to work the FTAG, FTHG, AST and HST features.\n",
    "These characteristics will not be present when the prediction is made, therefore we will \n",
    "create new characteristics based on these and that can accumulate in some way the values \n",
    "of the training.\n",
    "These new features will be: \n",
    "\n",
    "   - TFTG  accumulated - Full Time Home Team Goals\n",
    "   - TFTA  accumulated - Full Time Away Team Goals\n",
    "\n",
    "   - GAHT   accumulated - Goals against HomeTeam\n",
    "   - GAAT   accumulated - Goals against AwayTeam\n",
    "\n",
    "   - THST   Total - Home Team Shots on Target\n",
    "   - TAST   Total - Away Team Shots on Target\n",
    "\n",
    "   - HTP    Home Total Points accumulate\n",
    "   - ATP    Away Total Points accumulate \n",
    "\n",
    "   - HGA    HomeTeam Goal Average\n",
    "   - AGA    AwayTeam Goal Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "features = features_engineer(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics of the first week (the first 19 records) must be 0, \n",
    "since each team plays only once and does not have any antecedents to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The new features have the expected importance to train the model, \n",
    "remember that the statistics of the game know them after its completion, \n",
    "therefore you must work with the data prior to the game, \n",
    "this way we can now eliminate those characteristics that \"No we must use\":\n",
    "These are: \n",
    "    FTAG, FTHG, AST, HST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test\n",
    "We can not use the sklearn methods to perform the \"splits\", since we are working with time series and can not choose random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the characteristics for learning and testing\n",
    "cols = ['TFTHG','TFTAG', 'GAHT', 'GAAT','HGA','AGA','HTP','ATP',\n",
    "        'diffgoals','diffshots','diffp','diffg']\n",
    "\n",
    "# Amount of the test 10%, 90% Train\n",
    "n = 400     # record for the test\n",
    "\n",
    "# Split feature_table: Training (only the matches already played)\n",
    "X_train = features[cols]\n",
    "X_train = X_train[:-n] # Eliminate the last n rows, (nrows - n) for training\n",
    "\n",
    "# Split target Train: only the matches already played\n",
    "y_train = target[:-n] # Eliminate the last n rows, (nrows - n) for training\n",
    "\n",
    "\n",
    "# Split feature_table: Test (only the games not played)\n",
    "X_test = features[cols]\n",
    "X_test = X_test[features.shape[0]-n:] # n rows for the test\n",
    "\n",
    "# Split target test: only the games not played\n",
    "y_test = target[target.shape[0]-n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of the models\n",
    "We will start testing the following models:\n",
    "    - LogisticRegression\n",
    "    - xgboost\n",
    "    - KNeighbors\n",
    "    - SVC\n",
    "    - Decision Tree \n",
    "    - Random Forest\n",
    "    - AdaBoost\n",
    "    - Gradient Boosting\n",
    "    \n",
    "For the metrics we will use:\n",
    "    - f1 Score\n",
    "    - Cross Val Score\n",
    "    \n",
    "For the tuning:\n",
    "    - GridSearchCV\n",
    "    \n",
    "And finally: we will use the \"Stacking\" technique to try to improve the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_scores = dict()\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=12),\n",
    "    XGBClassifier(random_state=12),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(random_state = 42, kernel='rbf', gamma='auto',probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()]\n",
    "\n",
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"F1 Score\"]\n",
    "scores = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"-\"*82+\"\\n\")\n",
    "    print(\"Clasifier: \" + clf_name)\n",
    "    print('****Results****')\n",
    "\n",
    "    y_predict_test = clf.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc=accuracy_score(y_test, y_predict_test)\n",
    "    print(\"Accuracy Score: \" + str(round(100*acc, 2)) + \"% \\n\")\n",
    "\n",
    "    # F1 Scores\n",
    "    f1s = f1_score(y_test, y_predict_test, average='weighted')\n",
    "    print(\"f1 Score: \" + str(round(100 * f1s, 2)) + \"% \\n\")\n",
    "    \n",
    "    train_predictions = y_predict_test\n",
    "    log_entry = pd.DataFrame([[clf_name, acc, f1s ]], columns=log_cols)\n",
    "    scores = scores.append(log_entry)\n",
    "    \n",
    "print(\"-\"*82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Accuracy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=scores.sort_values(by='Accuracy', ascending = False))\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Classifier Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the F1 Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='F1 Score', y='Classifier', \n",
    "            data=scores.sort_values(by='F1 Score', ascending = False))\n",
    "\n",
    "plt.xlabel('F1 Score')\n",
    "plt.title('Classifier F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Accuracy Vs F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "x = np.arange(len(scores))\n",
    "bar_width = 0.4\n",
    "plt.bar(x, scores['Accuracy'], width = bar_width, color = 'orange', zorder=2)\n",
    "plt.bar(x + bar_width, scores['F1 Score'], width = bar_width, color = 'blue', zorder=2)\n",
    "\n",
    "plt.xticks(x + bar_width, scores['Classifier'], rotation='vertical')\n",
    "plt.title('Accuracy Vs F1 Score')\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Scores')\n",
    "\n",
    "orange_patch = mpatches.Patch(color='orange', label='Accuracy')\n",
    "blue_patch = mpatches.Patch(color='blue', label='F1 Score')\n",
    "\n",
    "plt.legend(handles=[orange_patch,blue_patch])\n",
    "\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best models (without tuning) are:\n",
    "\n",
    "Clasifier: SVC\n",
    "****Results****\n",
    "Accuracy Score: 50.25% \n",
    "f1 Score: 40.56% \n",
    "\n",
    "Clasifier: XGBClassifier\n",
    "****Results****\n",
    "Accuracy Score: 46.25% \n",
    "f1 Score: 40.38%\n",
    "\n",
    "Clasifier: LogisticRegression\n",
    "****Results****\n",
    "Accuracy Score: 47.5% \n",
    "f1 Score: 39.99% \n",
    "\n",
    "Clasifier: AdaBoostClassifier\n",
    "****Results****\n",
    "Accuracy Score: 45.5% \n",
    "f1 Score: 39.82% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above, I will take the best classifiers to optimize them:\n",
    "    - LogisticRegression\n",
    "    - XGBClassifier\n",
    "    - SVC\n",
    "    - AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models for GridSearchCV\n",
    "\n",
    "models = [{'name': 'Logistic Regression','label': 'Logistic Regression',\n",
    "           'classifier': LogisticRegression(random_state=49),\n",
    "           'grid': {'penalty':['l2'],\n",
    "                    'C':[5,6,7],\n",
    "                    'solver':['newton-cg','lbfgs','sag','saga'],\n",
    "                    'max_iter':[100],\n",
    "                    'multi_class':['ovr','multinomial']}},\n",
    "          \n",
    "          {'name': 'Xgboost','label':'Xgboost',\n",
    "           'classifier': XGBClassifier(random_state=49),\n",
    "           'grid':{'seed':[0],\n",
    "                   'n_estimators':[5,10,20],\n",
    "                   'learning_rate':[0.1],\n",
    "                   'subsample':[0.8, 0.9],\n",
    "                   'objective':['binary:logistic'],\n",
    "                   'max_depth':[2,3,4],\n",
    "                   'gamma':[1,2,3],\n",
    "                   'min_child_weight':[2,3,4]}},\n",
    "          \n",
    "          {'name': 'SVC (RBF)', 'label': 'SVC (RBF)',\n",
    "           'classifier': SVC(random_state=49, probability=True ),\n",
    "           'grid': {'C': [9,10],\n",
    "                    'gamma': [0.01, 0.001],\n",
    "                    'kernel': ['linear','rbf','poly','sigmoid']}},\n",
    "          \n",
    "          {'name': 'AdaBoost', 'label': 'AdaBoost Classifier',\n",
    "           'classifier':AdaBoostClassifier(learning_rate=1,\n",
    "                                           random_state=49),\n",
    "           'grid': {'n_estimators': [2,4,6],\n",
    "                    'algorithm':['SAMME','SAMME.R'] }}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(classifier, name, grid, X_train, y_train, X_test, y_test, scoring1, scoring2):\n",
    "    \n",
    "    gridsearch_cv1=GridSearchCV(classifier, \n",
    "                               grid,\n",
    "                               cv=5, \n",
    "                               scoring = scoring1)\n",
    "    \n",
    "    gridsearch_cv2=GridSearchCV(classifier, \n",
    "                               grid,\n",
    "                               cv=5, \n",
    "                               scoring = scoring2)\n",
    "    \n",
    "    gridsearch_cv1.fit(X_train, y_train)\n",
    "    gridsearch_cv2.fit(X_train, y_train)\n",
    "    \n",
    "    results_dict = {} # Scores with data training\n",
    "    \n",
    "    results_dict['classifier_name'] = name    \n",
    "    results_dict['classifier'] = gridsearch_cv1.best_estimator_\n",
    "    results_dict['best_params'] = gridsearch_cv1.best_params_\n",
    "    \n",
    "    # Traininig Scores\n",
    "    results_dict['accuracy-train'] = gridsearch_cv1.best_score_\n",
    "    results_dict['f1 score-train'] = gridsearch_cv2.best_score_\n",
    "\n",
    "    # Test Scores    \n",
    "    y_pred = gridsearch_cv1.best_estimator_.predict(X_test)\n",
    "    results_dict['accuracy-test'] = accuracy_score(y_test, y_pred, 'accuracy')\n",
    "    \n",
    "    y_pred = gridsearch_cv2.best_estimator_.predict(X_test)\n",
    "    results_dict['f1 score-test'] = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return(results_dict)\n",
    "\n",
    "results = []\n",
    "\n",
    "for mod in models:    \n",
    "    print(mod['name'], \".....\")    \n",
    "    results.append(model_selection(mod['classifier'], \n",
    "                                   mod['name'],\n",
    "                                   mod['grid'],\n",
    "                                   X_train, \n",
    "                                   y_train, \n",
    "                                   X_test,\n",
    "                                   y_test,\n",
    "                                   'accuracy',\n",
    "                                   'f1_weighted'))      \n",
    "    print('....ready ', mod['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(by='accuracy-test', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['classifier_name','accuracy-train','accuracy-test','f1 score-train','f1 score-test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = results_df[results_df['classifier_name'] == 'Xgboost'].best_params.values\n",
    "lr_params = results_df[results_df['classifier_name'] == 'Logistic Regression'].best_params.values\n",
    "svc_params = results_df[results_df['classifier_name'] == 'SVC (RBF)'].best_params.values\n",
    "ada_params = results_df[results_df['classifier_name'] == 'AdaBoost'].best_params.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"xgb_params: \", xgb_params, \"\\n\\n\",\n",
    "      \"lr_params: \", lr_params, \"\\n\\n\",\n",
    "      \"svc_params: \", svc_params, \"\\n\\n\",\n",
    "      \"ada_params: \", ada_params, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Classifiers after tuning (accuracy-test set):\n",
    "\n",
    "    - XgBoost, accuracy: 0.4950 \n",
    "    - SVC, accuracy: 0.4925 \n",
    "    - Logistic regression, accuracy: 0.4775\t \n",
    "    - AdaBoostClassifier, accuracy: 0.4750 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Classifiers after tuning (accuracy-training set):\n",
    "\n",
    "    - XgBoost, accuracy: 0.557059 \n",
    "    - Logistic regression, accuracy: 0.548529\t\n",
    "    - AdaBoostClassifier, accuracy: 0.540588\n",
    "    - SVC, accuracy: 0.533529\t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "To try to improve the accuracy we will use the technique of \"Stacking\"\n",
    "\n",
    "We will use the following models in the layers:\n",
    "    \n",
    "    - XgBoost\n",
    "    - LogisticRegression\n",
    "    - SVC\n",
    "    - AdaBoostClassifier\n",
    "    \n",
    "And as a last layer (similar to 'softmax') we will use LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "from mlens.ensemble import Subsemble\n",
    "from mlens.metrics.metrics import rmse\n",
    "\n",
    "def f1(y, p): return f1_score(y, p, average='weighted')\n",
    "\n",
    "ensemble = SuperLearner(scorer=f1, random_state=49, verbose=True)\n",
    "\n",
    "ensemble.add([XGBClassifier(gamma=2, \n",
    "                            learning_rate=0.1,\n",
    "                            max_depth=3,\n",
    "                            min_child_weight=4,\n",
    "                            n_estimators=10,\n",
    "                            objective='binary:logistic',\n",
    "                            seed=0,\n",
    "                            subsample=0.9)])\n",
    "\n",
    "ensemble.add([LogisticRegression(C=6, \n",
    "                                 max_iter=100, \n",
    "                                 multi_class='ovr', \n",
    "                                 penalty='l2', \n",
    "                                 solver='lbfgs', \n",
    "                                 random_state=49)])\n",
    "ensemble.add([SVC(C=10, \n",
    "                  gamma=0.001, \n",
    "                  kernel='rbf',\n",
    "                  random_state=49)])\n",
    "\n",
    "ensemble.add([AdaBoostClassifier(algorithm='SAMME.R', \n",
    "                                 n_estimators=6,\n",
    "                                 random_state=49)])\n",
    "\n",
    "\n",
    "ensemble.add_meta([LogisticRegression(C=6, \n",
    "                                 max_iter=100, \n",
    "                                 multi_class='ovr', \n",
    "                                 penalty='l2', \n",
    "                                 solver='lbfgs', \n",
    "                                 random_state=49)])\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "preds = ensemble.predict(X_test)\n",
    "f1(y_test, preds)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The stacking technique obtained the same accuracy as the best of the models (XgBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation with Times Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = features[cols]\n",
    "y = target\n",
    "\n",
    "tsv = TimeSeriesSplit(n_splits=3).split(X)\n",
    "\n",
    "clf = XGBClassifier(gamma=2, \n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=3,\n",
    "                    min_child_weight=4,\n",
    "                    n_estimators=10,\n",
    "                    objective='binary:logistic',\n",
    "                    seed=0,\n",
    "                    subsample=0.9)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=tsv, scoring='accuracy')\n",
    "    \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "### Our selected model has 0.53 and a very small variation of 0.01, which is acceptable\n",
    "### In this way we match the accuracy of the betting house programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
